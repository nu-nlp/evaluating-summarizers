import json
from typing import List, Tuple
from pathlib import Path
import argparse
import pandas as pd
from data_utils.data import datasets_mapping
import nltk
nltk.download("punkt")

from evaluation.extensions import (
    bleu_metric,
    sacrebleu_metric,
    rouge_metric,
    bertscore_metric,
    jensen_shannon_metric,
    avg_gen_length_metric
    # mauve_metric,
)

david_datasets_mapping = {
    "cnn_dailymail": "cnn",
    "arxiv": "arxiv",
    "pubmed": "pubmed",
    "reddit_tifu": "tifu",
    "billsum": "billsum",
    "govreport": "gov",
}

DATASETS = ["cnn_dailymail", "arxiv", "billsum", "govreport", "pubmed", "reddit_tifu"]
MODELS = ["TextRank", "LexRank", "Lead", "Random", "Occams", "bartbase", "bartlarge", "t5small", "t5base", "pegasuslarge", "pegasusxsum"]
METRICS = ["sacrebleu", "bleu", "rouge", "bertscore", "jensen_shannon", "avg_gen_length"]

def load_summarization_outputs(summary_file: Path,
                               summary_column: str,
                               target_column: str,
                               debug: bool) -> Tuple[List[str], List[str]]:

    # if summarizer in ["TextRank", "LexRank", "Lead", "Random", "Occams"]:
    #     # optimization based models outputs will be found with this code
    #     filename = "summarization_test_debug.csv" if debug else "summarization_test.csv"
    #     file = summarizations_dir / dataset / summarizer / filename
    # else:
    #     # To run this on David's CSVs: e.g. billsum_bartbase_197.csv
    #     filename = f"{david_datasets_mapping[dataset]}_{summarizer}_{datasets_mapping[dataset][4]}.csv"
    #     file = summarizations_dir / filename

    # Read the summarization outputs into  dataframe
    df = pd.read_csv(summary_file)

    # TextRank returns empty summaries if the document does not have enough sentences
    # It seems to occur with bad sentence tokenization and is probably fixable in the future
    # https://github.com/summanlp/textrank/issues/28
    # For now we replace empty summaries with empty strings.
    
    predictions = df[summary_column].fillna("").tolist()
    references = df[target_column].fillna("").tolist()
    
    # unit_summary_time = df['summarization_time']
    # summarization_time = unit_summary_time.mean()

    # if you want to debug, use debug flag to restrict to 5 samples
    if debug:
        predictions, references = predictions[:5], references[:5]

    return predictions, references

def evaluate_one_summary_file(summarizations_dir: Path,
                              training_dataset: str,
                              evaluation_dataset: str,
                              summarizer: str,
                              target_length: int,
                              results_dir: Path,
                              metrics: List[str],
                              summary_column: str,
                              target_column: str,
                              debug: bool) -> None:

    # Get the path of the for the summary csv file to evaluate
    # Note that the file format consists of four properties separated by underscores:
    #           <training_dataset>_<testing_dataset>_<model/method>_<target_length>.csv)
    summary_filename = f"{training_dataset}_{evaluation_dataset}_{summarizer}_{target_length}"
    full_summary_filename = summary_filename + ".csv"
    summary_filepath = summarizations_dir / full_summary_filename

    # Load the predictions and gold references from file
    predictions, references = load_summarization_outputs(summary_filepath, summary_column, target_column, debug)

    # Create a dictionary for the metric results
    results = {"summarizer": summarizer,
               "training_dataset": training_dataset,
               "evaluation_dataset": evaluation_dataset,
               "target_length": target_length,
               "summary_file": str(summary_filepath)}

    # Begin running the evaluation metrics
    print(f"Evaluating summaries generated by {summarizer} (trained on {training_dataset}) with a target length of {target_length} on evaluation dataset {evaluation_dataset}")
    for metric_name in metrics:
        # load metric
        if metric_name == "bleu":
            metric = bleu_metric

        elif metric_name == "sacrebleu":
            metric = sacrebleu_metric

        elif metric_name == "rouge":
            metric = rouge_metric

        elif metric_name == "bertscore":
            metric = bertscore_metric

        # TODO: NOT PRIORITY. Implement these two successfully
        # Mauve does not work yet
        # elif metric_name == "mauve":
        #   metric = mauve_metric

        elif metric_name == "jensen_shannon":
            metric = jensen_shannon_metric

        elif metric_name == "avg_gen_length":
            metric = avg_gen_length_metric

        # evaluate summaries
        scores = metric.evaluate(predictions=predictions, references=references)

        results[metric_name] = scores

    # Get the directory where the evaluation results will be saved and the filename for the results
    results_dir.mkdir(parents=True, exist_ok=True)
    results_filename = summary_filename
    if debug:
        results_filename += "_debug"
    if target_column == "label":
        results_filename += "_label"

    # Save the results
    final_output_path = results_dir / f"{results_filename}.json"
    with open(final_output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    print(f"Finished. Wrote evaluation results to: {final_output_path}")

    return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run evaluation metrics on summarization outputs.")

    parser.add_argument("--training_dataset", type=str, default="arxiv")
    parser.add_argument("--evaluation_dataset", type=str, default="arxiv")
    parser.add_argument("--summarizer", type=str, default="TextRank")
    parser.add_argument("--target_length", type=int, default=200)

    # Directory arguments
    parser.add_argument("--summarizations_dir", type=Path, default=Path("evaluation/generated_summaries"))
    parser.add_argument("--results_dir", type=Path, default=Path("evaluation/evaluation_results"))

    # Metrics arguments
    parser.add_argument("--metrics", type=str, nargs="*", default=["rouge", "sacrebleu", "bleu", "jensen_shannon", "avg_gen_length"])
    # parser.add_argument("--all_metrics", action="store_true")

    # Column arguments
    parser.add_argument("--summary_column", type=str, default="summary")
    parser.add_argument("--target_column", type=str, default="target")

    # Debug arguments
    parser.add_argument("--debug", action="store_true")

    # Parse the arguments and run the script
    args = parser.parse_args()
    results = evaluate_one_summary_file(**dict(args._get_kwargs()))
